{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWhat is Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression in statistics is the process of predicting a Label(or Dependent Variable) based on the features(Independent Variables) at hand.Here, we fit a curve/line to the data points, in such a manner that the differences between the distance of the actual data points from the plotted curve/line is minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWhat is Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a statistical method of finding the relationship between independent and dependent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tWhen to use Linear Regression? Explain the equation of a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Discreet/continuous independent variables\n",
    "* A best-fit regression line\n",
    "* Continuous dependent variable.\n",
    "i.e., A Linear Regression model predicts the dependent variable using a regression line based on the independent variables.\n",
    "The equation of the Linear Regression is:\n",
    "\n",
    "                                                Y=a+b*X + e \n",
    "\n",
    " Where,\n",
    " a is the intercept, \n",
    "b is the slope of the line, \n",
    "and e is the error term. \n",
    "The equation above is used to predict the value of the target variable based on the given predictor variable(s).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhat kind of plots will you use to showcase the relationship amongst the columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Scatterplot*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tHow is the best fit line chosen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The best fit line is obtained by minimizing the residual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tWhat is gradient descent, and why is it used? Explain the maths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, Residual is: $$r={y-(mx+b)}$$\n",
    "Hence, the sum of the square of residuals is:\n",
    "<img src=\"sumOfResiduals.png\" width=\"300\">\n",
    "\n",
    "As we can see that the residual is both a function of m and b, so differentiating partially with respect to m and b will give us:\n",
    "<img src=\"partialDerivatives.png\" width=\"300\">\n",
    "\n",
    "For getting the best fit line, residual should be minimum. The minima of a function occurs where the derivative=0. So, equating our corresponding derivatives to 0, we get:\n",
    "<img src=\"minima.png\" width=\"300\">\n",
    "\n",
    "This same equation can be written in matrix form as:\n",
    "<img src=\"matrix1.png\" width=\"300\">\n",
    "\n",
    "Ideally, if we'd have an equation of one dependent and one independent variable the minima will look as follows:\n",
    "<img src=\"minima2.png\" width=\"300\">\n",
    "\n",
    "But as the residual's minima is dependent on two variables m and b, it becomes a _Paraboloid_ and the appropriate m and b are calculated using _*Gradient Descent*_ as shown below:\n",
    "<img src=\"GradientDescent.gif\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tWhat are residuals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Residual is the distance between the actual Y and the predicted Y.\n",
    "Mathematically, Residual is:\n",
    "ùëü=ùë¶‚àí(ùëöùë•+ùëè)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tWhat is correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the change in one variable appears to be accompanied by a change in the other variable, the two variables are said to be correlated and this inter¬≠dependence is called correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tWhat is multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearity is situation when independant variables having strong correlation among themselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\tHow to detect multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detection\n",
    "* __Correlation Matrices and Plots:__ for correlation between all the X variables.\n",
    "        \n",
    "        This plot shows the extent of correlation between the independent variable. Generally, a correlation greater than 0.9 or less than -0.9 is to be avoided.\n",
    "    <img src=\"cor.PNG\" width=\"500\">\n",
    "* __Variance Inflation Factor:__ Regression of one X variable against other X variables.\n",
    "\n",
    "     VIF=$\\frac {1}{(1-R squared)}$\n",
    "\n",
    "            The VIF factor, if greater than 10 shows extreme correlation between the variables and then we need to take care of the correlation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.\tWhat are the remedies for multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remedies for Multicollinearity\n",
    "\n",
    "* **Do Nothing:** If the Correlation is not that extreme, we can ignore it. If the correlated variables are not used in solving our business question, they can be ignored.\n",
    "* **Remove One Variable**: Like in dummy variable trap\n",
    "* **Combine the correlated variables:** Like creating a seniority score based on Age and Years of experience\n",
    "* Principal Component Analysis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.\tWhat is the R-Squared Statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $R^2$ statistics\n",
    "\n",
    "The R-squared statistic provides a measure of fit. It takes the form of a proportion‚Äîthe proportion of variance\n",
    "explained‚Äîand so it always takes on a value between 0 and 1. \n",
    "In simple words, it represents how much of our data is being explained by our model. \n",
    "For example,  $R^2$ statistic = 0.75, it says that our model fits 75 % of the total data set.\n",
    "Similarly, if it is 0, it means none of the data points is being explained and a value of 1 represents 100% data explanation.\n",
    "Mathematically $R^2$ statistic is calculated as :\n",
    "<img src=\"RSquared.PNG\" width=\"300\">\n",
    "\n",
    "The closer the value of R2 is to 1 the better the model fits our data. If R2 comes below 0(which is a possibility) that means the model is so bad that it is performing even worse than the average best fit line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.\tWhat is an adjusted R-Squared Statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Mathematically, it is calculated as:\n",
    "                                        <img src=\"adjr2.PNG\">\n",
    "In the equation above, when p = 0, we can see that adjusted R2 becomes equal to R2.\n",
    "Thus, adjusted R2  will always be less than or equal to R2, and it penalises the excess of independent variables which do not affect the dependent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14.\tWhy do we use adj R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we increase the number of independent variables in our equation, the R2 increases as well. But that doesn‚Äôt mean that the new independent variables have any correlation with the output variable. In other words, even with the addition of new features in our model, it is not necessary that our model will yield better results but R2 value will increase. To rectify this problem, we use Adjusted R2 value which penalises excessive use of such features which do not correlate with the output data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15.\tWhy adj R-squared decreases when we use incompetent variables?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjusted R-squared adjusts for the number of terms in the model. Importantly, its value increases only when the new term improves the model fit more than expected by chance alone. The adjusted R-squared value actually decreases when the term doesn‚Äôt improve the model fit by a sufficient amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16.\tHow to interpret a Linear Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If the expense on TV ad is $50000, what will be the sales prediction for that market?\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1x$$\n",
    "$$y = 7.032594 + 0.047537 \\times 50$$\n",
    "\n",
    "### Interpreting the model\n",
    "\n",
    "How do we interpret the coefficient for spends on TV ad ($\\beta_1$)?\n",
    "- A \"unit\" increase in spends on a TV ad is **associated with** a 0.047537 \"unit\" increase in Sales.\n",
    "- Or, an additional $1,000  on TV ads is **translated to** an increase in sales by 47.53 Dollars.\n",
    "\n",
    "As an increase in TV ad expenditure is associated with a **decrease** in sales, $\\beta_1$ would be **negative**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17.\tWhat is the difference between fit, fit_transform and predict methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Transformers:**\n",
    "\n",
    "**fit() -** It is used for calculating the initial filling of parameters on the training data (like mean of the column values) and saves them as an internal objects state\n",
    "\n",
    "**transform() -** Use the above calculated values and return modified training data\n",
    "\n",
    "**fit_transform() -** It joins above two steps. Internally, it just calls first fit() and then transform() on the same data.\n",
    "\n",
    "**For Models:**\n",
    "\n",
    "**fit() -** It calculates the parameters/weights on training data (e.g. parameters returned by coef() in case of Linear Regression) and saves them as an internal objects state.\n",
    "\n",
    "**predict() -** Use the above calculated weights on test data to make the predictions\n",
    "\n",
    "**transform() -** Cannot be used\n",
    "\n",
    "**fit_transform() -** Cannot be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18.\tHow do you plot the least squared line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame with the minimum and maximum values of TV\n",
    "# make predictions for those x values and store them\n",
    "# first, plot the observed data\n",
    "# then, plot the least squares line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19.\tWhat are Bias and Variance? What is Bias Variance Trade-off?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Variance trade-off\n",
    "\n",
    "Variance is a measure of the degree of the spread of the predicted results. Bias, on the other hand, is a measure of the polarity of the results i.e., the results are favored towards a specific outcome.  The problem with Bias and Variance can be depicted as follows:\n",
    "\n",
    "<img src=\"bias_variance.PNG\" width=\"300\">\n",
    " \n",
    " As depicted in the figure, our goal is to minimize the Variance as well as Bias to get to accurate results.\n",
    "If there are lesser number of predictors in the data set that too having big coefficients, it is a possibility that the model obtained is favored towards those predictors i.e., a small change in those predictors lead to a big change in the predicted result. In other words, that predictor is driving our model, or our model is biased for that predictor. An easy way to overcome this is to introduce more no. of predictors which contributes towards the end result of the prediction. This increases the complexity of the model or in other words, the variance in the model increases.\n",
    "Mathematically,\n",
    "\n",
    "Total Error = $Bias^2 + Variance + Irreducible Error$\n",
    "\n",
    "Irreducible error is the error due to noise in the data.\n",
    "\n",
    "Graphically it looks like:\n",
    "\n",
    "<img src=\"tradeoff.PNG\" width=\"300\">\n",
    "\n",
    "It can be seen that as the bias decreases, the variance increases.\n",
    " So, we need to find a balance so that we get a model which has low variance as well as low bias. This is called **bias-variance tradeoff.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20.\tWhat is the null and alternate hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Null hypothesis:** The null hypothesis reflects that there will be no observed effect in our experiment. In a mathematical formulation of the null hypothesis, there will typically be an equal sign. This hypothesis is denoted by H0.\n",
    "- **Alternative hypothesis:** TThe Alternative Hypothesis\n",
    "The alternative or experimental hypothesis reflects that there will be an observed effect for our experiment. In a mathematical formulation of the alternative hypothesis, there will typically be an inequality, or not equal to symbol. This hypothesis is denoted by either Ha or by H1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21.\tWhat is multiple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression\n",
    "\n",
    "Multiple linear regression (MLR), also known simply as multiple regression, is a statistical technique that uses several explanatory variables to predict the outcome of a response variable.\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$\n",
    "\n",
    "Each $x$ represents a different feature, and each feature has its own coefficient. In this case:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 \\times TV + \\beta_2 \\times Radio + \\beta_3 \\times Newspaper$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22.\tWhat is the OLS method? Derive the formulae used in the OLS method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, ordinary least squares (OLS) is a type of linear least squares method for estimating the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the given dataset and those predicted by the linear function.\n",
    "\n",
    "\n",
    "Mathematically, Residual is: $$r={y-(mx+b)}$$\n",
    "Hence, the sum of the square of residuals is:\n",
    "<img src=\"sumOfResiduals.png\" width=\"300\">\n",
    "\n",
    "As we can see that the residual is both a function of m and b, so differentiating partially with respect to m and b will give us:\n",
    "<img src=\"partialDerivatives.png\" width=\"300\">\n",
    "\n",
    "For getting the best fit line, residual should be minimum. The minima of a function occurs where the derivative=0. So, equating our corresponding derivatives to 0, we get:\n",
    "<img src=\"minima.png\" width=\"300\">\n",
    "\n",
    "This same equation can be written in matrix form as:\n",
    "<img src=\"matrix1.png\" width=\"300\">\n",
    "\n",
    "Ideally, if we'd have an equation of one dependent and one independent variable the minima will look as follows:\n",
    "<img src=\"minima2.png\" width=\"300\">\n",
    "\n",
    "But as the residual's minima is dependent on two variables m and b, it becomes a _Paraboloid_ and the appropriate m and b are calculated using _*Gradient Descent*_ as shown below:\n",
    "<img src=\"GradientDescent.gif\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23.\tWhat is the p-value? How does it help in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value represents the probability of the coefficient actually being zero\n",
    "If the 95% confidence interval includes zero, the p-value for that coefficient will be greater than 0.05. If the 95% confidence interval does not include zero, the p-value will be less than 0.05.\n",
    "\n",
    "Thus, a p-value of less than 0.05 is a way to decide whether there is any relationship between the feature in consideration and the response or not. Using 0.05 as the cutoff is just a convention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24.\tHow to handle categorical values in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We encoding categorical values either manually( small data) or using encoding techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25.\tWhat is regularization, and why do we need it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use regression models to train some data, there is a good chance that the model will overfit the given training data set. Regularization helps sort this overfitting problem by restricting the degrees of freedom of a given equation i.e. simply reducing the number of degrees of a polynomial function by reducing their corresponding weights.\n",
    "In a linear equation, we do not want huge weights/coefficients as a small change in weight can make a large difference for the dependent variable (Y). So, regularization constraints the weights of such features to avoid overfitting.\n",
    "\n",
    "Regularization helps to reduce the variance of the model, without a substantial increase in the bias. If there is variance in the model that means that the model won‚Äôt fit well for dataset different that training data. The tuning parameter Œª controls this bias and variance tradeoff. When the value of Œª is increased up to a certain limit, it reduces the variance without losing any important properties in the data. But after a certain limit, the model will start losing some important properties which will increase the bias in the data. Thus, the selection of good value of Œª is the key. The value of Œª is selected using cross-validation methods. A set of Œª is selected and cross-validation error is calculated for each value of Œª and that value of Œª is selected for which the cross-validation error is minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26.\tExplain Ridge Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression (L2 Form)\n",
    "Ridge regression penalizes the model based on the sum of squares of magnitude of the coefficients. The regularization term is given by\n",
    "\n",
    " regularization=$ \\lambda *\\sum  |\\beta_j ^ 2| $\n",
    "\n",
    "Where, Œª is the shrinkage factor.\n",
    "\n",
    "and hence the formula for loss after regularization is:\n",
    "\n",
    "<img src=\"ridge.PNG\" width=\"300\">\n",
    "\n",
    "This value of lambda can be anything and should be calculated by cross validation as to what suits the model.\n",
    "\n",
    "Let‚Äôs consider $\\beta_1$ and $\\beta_2$ be coefficients of a linear regression and Œª = 1:\n",
    "\n",
    "For Lasso, $\\beta_1$ + $\\beta_2$ <= s  \n",
    "\n",
    "For Ridge, $\\beta_1^2$ + $\\beta_2^2$  <= s  \n",
    "\n",
    "Where s is the maximum value the equations can achieve\n",
    ".\n",
    "If we plot both the above equations, we get the following graph:\n",
    "\n",
    "<img src=\"ridge_vs_lasso.PNG\" width=\"300\">\n",
    "\n",
    "The red ellipse represents the cost function of the model, whereas the square (left side) represents the Lasso regression and the circle (right side) represents the Ridge regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27.\tExplain Lasso Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO(Least Absolute Shrinkage and Selection Operator) Regression (L1 Form)\n",
    "LASSO regression penalizes the model based on the sum of magnitude of the coefficients. The regularization term is given by\n",
    "\n",
    " regularization=$ \\lambda *\\sum  |\\beta_j| $\n",
    "\n",
    "Where, Œª is the shrinkage factor.\n",
    "\n",
    "and hence the formula for loss after regularization is:\n",
    "\n",
    "<img src=\"L1.PNG\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28.\tExplain Elastic Net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Elastic Net\n",
    "\n",
    "According to the Hands-on Machine Learning book, elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso‚Äôs regularization terms, and you can control the mix ratio Œ±. \n",
    "\n",
    "<img src=\"elasticNet.PNG\" width=\"300\">\n",
    "where Œ± is the mixing parameter between ridge (Œ±‚ÄÑ=‚ÄÑ0) and lasso (Œ±‚ÄÑ=‚ÄÑ1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29.\tWhy do we do a train test split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train-test split procedure is used to estimate the performance of machine learning algorithms when they are used to make predictions on data not used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30.\tWhat is polynomial regression? When to use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "For understanding Polynomial Regression, let's first understand a polynomial.\n",
    "Merriam-webster defines a polynomial as:\n",
    "\"_A mathematical expression of one or more algebraic terms each of which consists of a constant multiplied by one or more variables raised to a non-negative integral power (such as a + bx + cx^2)\"._\n",
    "Simply said, poly means many. So, a polynomial is an aggregation of many monomials(or Variables).\n",
    "A simple polynomial equation can be written as:\n",
    "$$y = {a+bx + cx^2+...+nx^n+...}$$\n",
    "\n",
    "So, Polynomial Regression can be defined as a mechanism to predict a _dependent variable_ based on the polynomial relationship with the _independent variable_.\n",
    "\n",
    " In the equation, _$$y= {a+bx + cx^2+...+nx^n+...}$$_ the maximum power of 'x' is called the degree of the polynomial equation.\n",
    " For example, if the degree is 1, the equation becomes $$y={a+bx}$$ which is a simple linear equation.\n",
    "              if the degree is 2, the equation becomes $$y = {a+bx + cx^2}$$ which is a quadratic equation and so on.\n",
    "              \n",
    "## When to use Polynomial Regression?\n",
    "Many times we may face a requirement where we have to do a regression, but when we plot a graph between a dependent and independent variables, the graph doesn't turn out to be a linear one.\n",
    "A linear graph typically looks like:\n",
    "<img src=\"LinearGraph.png\" width=\"300\">\n",
    "\n",
    "But what if the relationship looks like:\n",
    "<img src=\"PolynomialGraph.png\" width=\"300\">\n",
    "\n",
    "It means that the relationship between X and Y can't be described Linearly.\n",
    "Then comes the time to use the Polynomial Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31.\tExplain the steps for GCP deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Go to https://cloud.google.com/ and create an account if already haven‚Äôt created one. Then go to the console of your account.\n",
    "* Go to IAM and admin(highlighted) and click manage resources\n",
    "<img src=\"iam.PNG\" width= \"300\">\n",
    "\n",
    "* Click CREATE PROJECT to create a new project for deployment.\n",
    "* Once the project gets created, select App Engine and select Dashboard.\n",
    "<img src=\"dashboard.PNG\" width= \"300\">\n",
    "\n",
    "* Go to https://dl.google.com/dl/cloudsdk/channels/rapid/GoogleCloudSDKInstaller.exe to download the google cloud SDK to your machine.\n",
    "* Click Start Tutorial on the screen and select Python app and click start.\n",
    "<img src=\"tutorial.PNG\" width= \"300\">\n",
    "\n",
    "* Check whether the correct project name is displayed and then click next.\n",
    "<img src=\"setup.PNG\" width= \"300\">\n",
    "\n",
    "* Create a file ‚Äòapp.yaml‚Äô and put  ‚Äòruntime: python37‚Äô in that file.\n",
    "* Create a ‚Äòrequirements.txt‚Äô file by opening the command prompt/anaconda prompt, navigate to the project folder and enter the command ‚Äòpip freeze > requirements.txt‚Äô.\n",
    "It is recommended to use separate environments for different projects.\n",
    "* Your python application file should be called ‚Äòmain.py‚Äô. It is a GCP specific requirement.\n",
    "* Open command prompt window, navigate to the project folder and enter the command gcloud init to initialise the gcloud context.\n",
    "* It asks you to select from the list of available projects.\n",
    "<img src=\"select_project.PNG\" width= \"300\">\n",
    "\n",
    "* Once the project name is selected, enter the command gcloud app deploy app.yaml \n",
    "--project <project name>\n",
    "    \n",
    "* After executing the above command, GCP will ask you to enter the region for your application. Choose the appropriate one.\n",
    "<img src=\"region_select.PNG\" width= \"300\">\n",
    "\n",
    "* GCP will ask for the services to be deployed. Enter ‚Äòy‚Äô to deploy the services.\n",
    "* And then it will give you the link for your app,and the deployed app looks like:\n",
    "\n",
    "<img src=\"final_snap.PNG\" width= \"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32. What difficulties did you face in cloud deployment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Google cloud account activation not work due payment related issue.\n",
    "so i deploy it on heroku platform. \n",
    "difficulties face during deployment is:\n",
    "\n",
    "1) we run *pip freeze > requirements.txt* . The requirement file contain specific path for libraries instead of it's version.\n",
    "\n",
    "2) libraries like mkl-fft,mkl-random, mkl-services create problem during push to heroku master, so we remove it and it work.\n",
    "\n",
    "https://probably-of-getting-admission.herokuapp.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
